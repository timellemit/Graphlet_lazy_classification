{\rtf1\ansi\ansicpg1251\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fnil\fcharset0 Monaco;}
{\colortbl;\red255\green255\blue255;\red255\green0\blue0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh15480\viewkind0
\deftab720
\pard\pardeftab720

\f0\fs36 \cf2 pydev debugger: starting (pid: 4930)\cf0 \
parsing time:  4.61\
parsing time:  5.41\
True labels: \
[0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1]\
Lazy prediction: \
[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0]\
SVM prediction: \
[0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0]\
             precision    recall  f1-score   support\
\
          0       0.58      0.88      0.70         8\
          1       0.75      0.38      0.50         8\
\
avg / total       0.67      0.62      0.60        16\
\
[[7 1]\
 [5 3]]\
             precision    recall  f1-score   support\
\
          0       0.33      0.25      0.29         8\
          1       0.40      0.50      0.44         8\
\
avg / total       0.37      0.38      0.37        16\
\
[[2 6]\
 [4 4]]\
parsing time:  3.67\
parsing time:  5.18\
\cf2 pydev debugger: Unable to find module to reload: "tests.svm_lazy_comparison_whole_train".\cf0 \
True labels: \
[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\
Lazy prediction: \
[1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\
SVM prediction: \
[1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0]\
             precision    recall  f1-score   support\
\
          0       0.57      0.36      0.44        11\
          1       0.22      0.40      0.29         5\
\
avg / total       0.46      0.38      0.39        16\
\
[[4 7]\
 [3 2]]\
             precision    recall  f1-score   support\
\
          0       0.50      0.27      0.35        11\
          1       0.20      0.40      0.27         5\
\
avg / total       0.41      0.31      0.33        16\
\
[[3 8]\
 [3 2]]\
parsing time:  4.33\
parsing time:  3.97\
\cf2 pydev debugger: Unable to find module to reload: "tests.svm_lazy_comparison_whole_train".\cf0 \
\cf2 pydev debugger: Unable to find module to reload: "tests.svm_lazy_comparison_whole_train".\cf0 \
True labels: \
[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\
Lazy prediction: \
[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]\
SVM prediction: \
[0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1]\
\cf2 /Library/Python/2.7/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\cf0 \
\cf2   'recall', 'true', average, warn_for)\cf0 \
             precision    recall  f1-score   support\
\
         -1       0.00      0.00      0.00         0\
          0       0.86      0.92      0.89        13\
          1       1.00      0.33      0.50         3\
\
avg / total       0.88      0.81      0.82        16\
\
[[ 0  0  0]\
 [ 1 12  0]\
 [ 0  2  1]]\
             precision    recall  f1-score   support\
\
          0       0.75      0.46      0.57        13\
          1       0.12      0.33      0.18         3\
\
avg / total       0.63      0.44      0.50        16\
\
[[6 7]\
 [2 1]]\
parsing time:  4.43\
parsing time:  5.79\
True labels: \
[1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1]\
Lazy prediction: \
[0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\
SVM prediction: \
[0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 0]\
             precision    recall  f1-score   support\
\
          0       0.42      0.83      0.56         6\
          1       0.75      0.30      0.43        10\
\
avg / total       0.62      0.50      0.48        16\
\
[[5 1]\
 [7 3]]\
             precision    recall  f1-score   support\
\
          0       0.30      0.50      0.37         6\
          1       0.50      0.30      0.37        10\
\
avg / total       0.42      0.38      0.37        16\
\
[[3 3]\
 [7 3]]\
}